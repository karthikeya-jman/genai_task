{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import google.generativeai as genai\n",
    "import numpy as np\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.boeing.com\",\n",
    "\"https://www.goldmansachs.com\",\n",
    "# \"https://www.amazon.com\",\n",
    "\"https://corporate.exxonmobil.com\",\n",
    "\"https://www.hsbc.com\",\n",
    "\"https://www.sony.com\",\n",
    "# \"https://www.mcdonalds.com\",\n",
    "\"https://www.volkswagenag.com\",\n",
    "\"https://www.ibm.com\",\n",
    "\"https://www.unilever.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "apikey = os.getenv('API_KEY')\n",
    "genai.configure(api_key=apikey)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "            'temperature': 0.3,\n",
    "            'max_output_tokens': 1024\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genai_model(final_prompt):\n",
    "    model_response = model.generate_content(\n",
    "        final_prompt,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "    return model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ''' \n",
    "What is the company's mission statement or core values?\n",
    "What products or services does the company offer?\n",
    "When was the company founded, and who were the founders?\n",
    "Where is the company's headquarters located?\n",
    "Who are the key executives or leadership team members?\n",
    "Has the company received any notable awards or recognitions?'''\n",
    "\n",
    "routes_prompt =  f\"\"\" \n",
    "Assume you are a experienced data scientist and you have been given a task to filter routes of a companies website\n",
    "\n",
    "i will give list of urls enclosed in triple backticks and you will have to filter the routes of the website based on the following tasks, don't give all urls, try to find by generalizing\n",
    "{tasks}\n",
    "filtered routes should only contain the routes that relevant to the specified tasks\n",
    "output should be a list of filtered routes, dont give any other information\n",
    "\"\"\"\n",
    "\n",
    "extraction_prompt = f\"\"\"\n",
    "Assume you are an experienced data scientist tasked with extracting key information from a company's website. \n",
    "Below, I am providing the scraped data of the website along with the tasks that need to be completed. \n",
    "\n",
    "The tasks are:\n",
    "{tasks}\n",
    "\n",
    "Your job is to extract the required information from the scraped data and provide the output in the following format:\n",
    "- A dictionary with keys 'task_0', 'task_1', ..., where each task's key corresponds to its index.\n",
    "- If the information for a task is found, include the extracted information as the value.\n",
    "- If the information for a task is not found, return 'not found' as the value.\n",
    "\n",
    "Do not include any additional explanations or details.\n",
    "Focus only on answering the questions specified in the tasks based on the provided scraped data.\n",
    "\n",
    "The output should be in this format:\n",
    "{{\n",
    "    'task_0': 'extracted information',\n",
    "    'task_1': 'extracted information',\n",
    "    ...\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "extraction_prompt_2 = f\"\"\"\n",
    "Assume you are an experienced data scientist tasked with extracting key information from company websites. Your goal is to precisely extract information for these specific tasks:\n",
    "\n",
    "{tasks}\n",
    "\n",
    "Rules for Extraction:\n",
    "- Use ONLY the information provided in the scraped data\n",
    "- If information is not found, use the exact phrase \"not found\"\n",
    "- Be literal and precise in extraction\n",
    "- Do not add, interpret, or modify the extracted data\n",
    "- Return results in a dictionary format with keys 'task_0' through 'task_5'\n",
    "- Ensure each task's information is extracted verbatim from the source\n",
    "\n",
    "Output Format:\n",
    "{{\n",
    "    'task_0': 'Mission statement or core values',\n",
    "    'task_1': 'Products and services',\n",
    "    'task_2': 'Founding year; founders',\n",
    "    'task_3': 'Headquarters location',\n",
    "    'task_4': 'Executives and leadership',\n",
    "    'task_5': 'Awards and recognitions'\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    # Create a session for persistent connections\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Set headers to mimic a browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive'\n",
    "    }\n",
    "    \n",
    "    # Send the GET request with headers\n",
    "    response = session.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Clean the HTML content: remove scripts, tags, and extra spaces\n",
    "    page_text = str(soup)\n",
    "    page_text = re.sub(r'<script.*?>.*?</script>', '', page_text, flags=re.DOTALL)  # Remove <script> tags\n",
    "    page_text = re.sub(r'<.*?>', '', page_text)  # Remove all HTML tags\n",
    "    page_text = re.sub(r'\\s+', ' ', page_text)  # Replace multiple spaces with a single space\n",
    "    page_text = page_text.strip()  # Remove leading and trailing spaces\n",
    "    \n",
    "    return page_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_extracted_data(raw_text):\n",
    "    # Remove code block markers and extra newlines\n",
    "    cleaned_text = raw_text.replace('```json\\n', '').replace('\\n```', '').strip()\n",
    "    \n",
    "    try:\n",
    "        # Parse the JSON\n",
    "        data = json.loads(cleaned_text)\n",
    "        \n",
    "        # Clean each key's value\n",
    "        for key in data:\n",
    "            # Remove extra newlines and multiple spaces\n",
    "            if isinstance(data[key], str):\n",
    "                data[key] = ' '.join(data[key].split())\n",
    "        \n",
    "        return data\n",
    "    except json.JSONDecodeError:\n",
    "        # If JSON parsing fails, try to extract JSON manually\n",
    "        try:\n",
    "            # Use regex to extract JSON content\n",
    "            json_match = re.search(r'\\{.*\\}', cleaned_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                cleaned_text = json_match.group(0)\n",
    "                return json.loads(cleaned_text)\n",
    "        except:\n",
    "            print(\"Failed to parse JSON\")\n",
    "            return None\n",
    "\n",
    "def save_to_csv(results, output_file='company_data.csv'):\n",
    "    # Prepare the data for CSV\n",
    "    csv_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        # Process each result\n",
    "        cleaned_result = clean_extracted_data(result['data'])\n",
    "        \n",
    "        if cleaned_result:\n",
    "            # Create a row with URL and all tasks\n",
    "            row = {\n",
    "                'URL': result.get('url', 'Unknown'),\n",
    "                'Task_0_Mission': cleaned_result.get('task_0', 'Not Found'),\n",
    "                'Task_1_Products': cleaned_result.get('task_1', 'Not Found'),\n",
    "                'Task_2_Founding': cleaned_result.get('task_2', 'Not Found'),\n",
    "                'Task_3_Headquarters': cleaned_result.get('task_3', 'Not Found'),\n",
    "                'Task_4_Executives': cleaned_result.get('task_4', 'Not Found'),\n",
    "                'Task_5_Awards': cleaned_result.get('task_5', 'Not Found')\n",
    "            }\n",
    "            csv_data.append(row)\n",
    "    \n",
    "    # Convert to DataFrame and save to CSV\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = []\n",
    "# Iterate through the list of URLs\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Fetch the page content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all routes (URLs) from <a> tags\n",
    "        routes = []\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                routes.append(href)\n",
    "\n",
    "        # Remove duplicate routes by converting the list to a set and back to a list\n",
    "        unique_routes = list(set(routes))\n",
    "\n",
    "        model_input = f\"{routes_prompt} ```{unique_routes}```\"\n",
    "        model_response = genai_model(model_input)\n",
    "\n",
    "        route_pattern = r\"'/([^']+)'\"  # Pattern to match content between single quotes\n",
    "        extracted_routes = re.findall(route_pattern, model_response.text)\n",
    "\n",
    "        all = \"\"\n",
    "        for i in extracted_routes:\n",
    "            try:\n",
    "                all += scrape(url + \"/\" + i)\n",
    "            except Exception as e:\n",
    "                log_message = f\"Error scraping route {url}/{i}: {str(e)}\\n\"\n",
    "                open(\"output.txt\", \"a\").write(log_message)\n",
    "                print(log_message)\n",
    "\n",
    "        b = genai_model(f\"{extraction_prompt_2} {all}\").text\n",
    "        open(\"output.txt\", \"a\").write(b)\n",
    "        print(b)\n",
    "        result_dict = {\n",
    "            'url': url,\n",
    "            'data': b\n",
    "        }\n",
    "        final_res.append(result_dict)\n",
    "    except Exception as e:\n",
    "        log_message = f\"Error scraping URL {url}: {str(e)}\\n\"\n",
    "        open(\"output.txt\", \"a\").write(log_message)\n",
    "        print(log_message)\n",
    "\n",
    "# save_to_csv(final_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(final_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
